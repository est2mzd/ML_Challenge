{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TransFuser\n",
    "### Imitation with Transformer-Based Sensor Fusion for Autonomous Driving\n",
    "[This paper is here.](https://arxiv.org/abs/2205.15997)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# この論文でやっていること\n",
    "### 入力データ\n",
    "1. 画像 RGB\n",
    "2. Lidarの点群データ(xyz)\n",
    "3. CARLAからもらうグローバルプランニング（経路の点群xyz）の中から、数秒先の1点\n",
    "\n",
    "### 出力データ\n",
    "1. ローカルプランニング（自車座標系における、4点の時系列座標xyz）\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatGPTの解説-1\n",
    "## Transfuserの論文を、章ごとに、日本語で要約してください\n",
    "\n",
    "以下は、論文「TransFuser: Imitation with Transformer-Based Sensor Fusion for Autonomous Driving」の章ごとの日本語要約です。\n",
    "\n",
    "### 1. はじめに\n",
    "この章では、LiDARセンサーが自動運転車に正確な3D情報を提供することの重要性と課題について述べています。特に、既存のセンサーフュージョン手法に基づく模倣学習が複雑な運転シナリオでの性能に限界があることを指摘し、新しいアプローチであるTransFuserを提案しています。このアプローチは、画像とLiDARの表現を自己注意機構を用いて統合し、複数の解像度でのセンサーフュージョンを実現します。\n",
    "\n",
    "### 2. 関連研究\n",
    "ここでは、マルチモーダル自動運転とセンサーフュージョンに関する既存の研究を概観しています。RGB画像に深度やセマンティクスを加えることで運転性能が向上することが示されていますが、既存の手法には高いインフラクション率やルート完遂率の低さが課題として残っています。これを踏まえ、TransFuserは注意機構を用いて複数の段階で異なるモダリティの情報を統合する新しいアプローチを提案します。\n",
    "\n",
    "### 3. TransFuser\n",
    "この章では、提案されたTransFuserアーキテクチャの詳細が説明されています。TransFuserは、複数のセンサーモダリティ（画像とLiDAR）の情報を統合するマルチモーダルフュージョントランスフォーマーと、自動回帰ウェイポイント予測ネットワークの二つの主要コンポーネントから成り立っています。問題設定として、都市環境でのポイント・ツー・ポイントナビゲーションが取り上げられています。\n",
    "\n",
    "### 4. 実験\n",
    "この章では、実験の設定、TransFuserの運転性能といくつかのベースラインとの比較、注意マップの可視化、アブレーションスタディが紹介されています。実験結果として、TransFuserは既存の手法に比べて大幅に優れた運転スコアを達成し、特に衝突の平均回数を48%削減することが確認されました。\n",
    "\n",
    "### 5. 結論\n",
    "最終章では、提案されたTransFuserの有効性と、今後の研究の方向性について述べています。TransFuserは、複雑な運転シナリオにおいても高い性能を示し、既存の手法を大きく上回る結果を達成しました。将来的には、さらに多くのセンサーや環境変数を統合することで、より安全で効率的な自動運転システムの開発が期待されます。\n",
    "\n",
    "この要約は、元の論文の主要なポイントを抽出して簡潔にまとめたものです。詳細な技術的内容や実験結果については、論文全文を参照してください。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatGPTの解説-2\n",
    "## この論文は、従来の論文のどのような課題を、どのような手段で克服したのですか？\n",
    "\n",
    "この論文「TransFuser: Imitation with Transformer-Based Sensor Fusion for Autonomous Driving」は、従来のセンサーフュージョンに基づく模倣学習の課題を克服するために、新しいアプローチを提案しています。以下は、その課題と克服手段についての詳細な説明です。\n",
    "\n",
    "## 課題\n",
    "\n",
    "### センサーフュージョンの限界:\n",
    "従来のセンサーフュージョン手法は、複雑な都市環境での自動運転において、情報を効果的に統合するのが難しいとされています。特に、局所的な情報に依存するため、動的なエージェント（他の車両や歩行者）との相互作用を正確に捉えることが難しいという問題があります。\n",
    "\n",
    "### 模倣学習の性能不足:\n",
    "従来の模倣学習モデルは、単一のモダリティ（例えば、カメラのみ、またはLiDARのみ）に依存しており、複数のセンサーから得られる情報を統合する能力が不足しているため、運転性能が制約されていました。\n",
    "\n",
    "### グローバルコンテキストの欠如:\n",
    "従来の手法では、シーン全体のグローバルコンテキストを捉えることが困難であり、特に交差点や複数レーンが交差する状況では、その欠如が顕著でした。\n",
    "\n",
    "## 克服手段\n",
    "\n",
    "### Transformerベースのフュージョン:\n",
    "この研究では、自己注意機構を用いるTransformerアーキテクチャを採用し、複数の解像度で画像とLiDARデータを融合します。これにより、各センサーから得られる情報を効果的に統合し、シーン全体のグローバルコンテキストを捉えることができます。\n",
    "\n",
    "### マルチモーダルデータの統合:\n",
    "提案されたTransFuserモデルは、画像とLiDARの両方のデータを使用し、それぞれのモダリティからの特徴を融合します。これにより、各モダリティの強みを活かしつつ、欠点を補完し合うことが可能となります。\n",
    "\n",
    "### 補助タスクの導入:\n",
    "模倣学習のトレーニングにおいて、深度予測、セマンティックセグメンテーション、HDマップ予測、および車両検出といった補助タスクを導入することで、モデルの解釈性と頑健性を向上させています。これにより、運転シーンの詳細な理解が促進され、運転性能が向上します。\n",
    "\n",
    "### 実験的な検証:\n",
    "提案されたモデルは、CARLAシミュレーターを用いた厳しい評価基準で実験的に検証されており、その結果、従来の最先端手法よりも優れた運転スコアと低いインフラクション率を達成しています。\n",
    "\n",
    "以上の手段により、この論文は従来のセンサーフュージョンと模倣学習の課題を克服し、複雑な運転シナリオでの自動運転性能を大幅に向上させています。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatGPTの解説-3\n",
    "## この論文が抱えている課題は何ですか？\n",
    "\n",
    "### 1. 複雑な交通シナリオでの車両衝突\n",
    "TransFuserは複雑な都市環境での運転性能を大幅に向上させましたが、特に高密度の交通シナリオにおいて車両衝突が依然として多く発生します。特に無保護左折や車線変更の際に、多くの衝突が発生することが観察されています。\n",
    "\n",
    "### 2. 運転環境の多様性への対応\n",
    "評価は主にCARLAシミュレーターを用いて行われていますが、シミュレーションと実際の運転環境との間にはギャップが存在します。実際の運転環境での多様な条件や予期しない状況への対応能力についてはさらなる検証が必要です。\n",
    "\n",
    "### 3. モデルの複雑さと計算コスト\n",
    "Transformerベースのアーキテクチャは高い計算コストを伴います。特に複数の解像度でのセンサーフュージョンを行うため、実行時の計算リソースと時間が大幅に増加する可能性があります。このため、リアルタイムの運転シナリオにおいて、効率性の向上が求められます。\n",
    "\n",
    "### 4. 他のセンサーとの統合\n",
    "現在のモデルは主にカメラとLiDARデータを統合していますが、他のセンサー（例えば、レーダー、GPS、IMUなど）との統合についてはさらなる研究が必要です。これらのセンサーを統合することで、より包括的な運転環境の理解が可能となると期待されます。\n",
    "\n",
    "### 5. 学習データの多様性\n",
    "模倣学習に使用されるデータセットの多様性が不十分である可能性があります。特に、異なる都市、気象条件、交通ルールなど、多様な運転シナリオに対応するためには、より広範なデータセットが必要です。\n",
    "\n",
    "### 6. 安全性の向上\n",
    "現在の安全性チェック（例：クリーピング中のLiDARヒット）は基本的なものであり、より高度な安全性メカニズムの導入が求められます。例えば、より精密なリスク評価や予測モデリングの統合などが考えられます。\n",
    "\n",
    "これらの課題に対処するためには、さらなる研究と実験が必要です。特に、実際の運転環境での検証や、他のセンサーとの統合、計算効率の向上などが重要な研究課題として挙げられます。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatGPTの解説-4\n",
    "## Transfuserの入力は何ですか？\n",
    "\n",
    "### 1. Imageの入力詳細\n",
    "#### カメラの構成:\n",
    " - 3台のカメラが使用され、それぞれ前方、左60度、右60度をカバーします。\n",
    "#### 解像度:\n",
    " - 各カメラの元の解像度は960×480ピクセルです。\n",
    " - 画像の歪みを取り除くために、960×480ピクセルの画像を320×160ピクセルにクロップします。\n",
    "#### 視野:\n",
    " - 3つのカメラを組み合わせることで、合計132度の視野をカバーします。\n",
    "#### 合成画像の解像度:\n",
    " - クロップされた3つの画像を横に並べて合成し、最終的な入力画像の解像度は320×704ピクセルになります。\n",
    "#### 最終的なImageの入力\n",
    " - 解像度: 320×704ピクセル\n",
    " - 視野: 132度\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. LiDAR入力の詳細\n",
    "#### 範囲:\n",
    " - LiDARデータは、車両の前方32メートル、左右16メートルの範囲をカバーします。  \n",
    " - この範囲は32m×32mの領域を構成します。\n",
    "  \n",
    "#### 解像度:\n",
    " - この32m×32mの領域は、0.125m×0.125mのグリッドに分割され、256×256ピクセルの解像度になります。\n",
    "\n",
    "#### 高度ヒストグラム:\n",
    " - LiDARポイントクラウドのデータを2つのビン（bin）に分けて、高度ヒストグラムを作成します。\n",
    " - 1つ目のビンは地面上のポイント、2つ目のビンは地面以下のポイントを表します。\n",
    "\n",
    "#### 目標位置:\n",
    " - 目標位置の情報を同じ256×256ピクセルのBEV（Bird's Eye View）空間でラスタライズし、高度ヒストグラムに追加します。\n",
    "\n",
    "#### 最終的なLiDAR入力\n",
    " - 以下の3つのチャンネルからなる256×256ピクセルの疑似画像として表現されます。\n",
    " - 範囲: 前方32メートル、左右16メートル（32m×32mの領域）\n",
    " - 解像度: 256×256ピクセル\n",
    " - チャンネル数: 3チャンネル（地面上ビン、地面以下ビン、目標位置）\n",
    "\n",
    "---\n",
    "\n",
    "#### Lidarの詳細な処理手順\n",
    "1. LiDARポイントクラウドの取得:\n",
    "   - LiDARセンサーから取得したポイントクラウドデータを使用します。\n",
    "   - Lidarからは、 x, y, z, 反射強度の4種のデータが得られる。\n",
    "   - ここでは、強度を無視して、x,y,zのみを使用している。\n",
    "\n",
    "2. ポイントクラウドのBEVグリッドへの変換:\n",
    "   - ポイントクラウドデータを2DのBEV（Bird's Eye View）グリッドに変換します。\n",
    "   - このグリッドは前方32メートル、左右16メートルの範囲をカバーします。\n",
    "   - x,y　を参照して、 32m x 32m の範囲外のデータを除去\n",
    "\n",
    "3. 高度ヒストグラムの作成:\n",
    "4. - z を使って、地面上　と　地面下　に分類する。\n",
    "   - BEVグリッド上の各セル（0.125m×0.125m）について、地面上のポイントと地面以下のポイントを数え、それぞれを2つのビンに分けます。\n",
    "   - 地面上と地面下のチャンネルを作成する理由は以下の通りです。\n",
    "     - 高さ情報の活用: 物体の識別と地形の理解を向上させるため。\n",
    "     - 障害物検出と回避: 地面上の障害物と地面下の特性を区別することで、正確な障害物回避が可能になるため。\n",
    "     - モデル精度の向上: 詳細な環境マッピングと複数の視点からの情報統合により、モデルの判断精度が向上するため。\n",
    "\n",
    "5. 目標位置のラスタライズ:\n",
    "   - 目標位置の情報を、BEVグリッドと同じ256×256ピクセルの解像度でラスタライズし、高度ヒストグラムに追加します。\n",
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3台のカメラ画像の合成方法\n",
    "#### カメラの構成と画像の合成方法\n",
    "1. カメラの配置:\n",
    "   - 自動運転車には3台のカメラが設置されています。\n",
    "   - 前方カメラ: 車両の前方を撮影します。\n",
    "   - 左カメラ: 左60度の視野をカバーします。\n",
    "   - 右カメラ: 右60度の視野をカバーします。\n",
    "\n",
    "2. 各カメラの画像解像度:\n",
    "   - 各カメラから取得される元の画像解像度は960×480ピクセルです。\n",
    "\n",
    "3. 画像のクロップ:\n",
    "   - 各カメラ画像は、画像の歪みを取り除くために320×160ピクセルにクロップされます。これにより、中央部分の重要な情報が保持されます。\n",
    "\n",
    "4. 画像の合成:\n",
    "   - クロップされた3つの画像（前方、左、右）を横に並べて1つのパノラマ画像を作成します。\n",
    "   - これにより、最終的な合成画像の解像度は320×704ピクセルになります（320ピクセルの高さ、704ピクセルの幅）。\n",
    "  ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatGPTの解説-5\n",
    "## Transfuserの出力は何ですか？\n",
    "\n",
    "### 1. 時系列のウェイポイント\n",
    "  - モデルは、将来の複数の時点におけるウェイポイントを予測します。\n",
    "  - 例えば、次の4つの時間ステップにおけるウェイポイント（w1, w2, w3, w4）を出力します。\n",
    "\n",
    "### 補足：　ウェイポイントとは\n",
    "  - 各ウェイポイントは、車両の現在の位置を基準とした相対座標（x, y）で表されます。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "# Check GPU\n",
    "import torch \n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfuser のブロック図\n",
    "\n",
    "![TransFuserのブロック図](../Images/Transfuser01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![TransFuserのブロック図](../Images/Transfuser03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下図の Image ブランチに関する補足\n",
    "### 入力チャンネル = RGB = 3\n",
    "  - チャンネル1：R\n",
    "  - チャンネル2：G\n",
    "  - チャンネル3：B\n",
    "\n",
    "### 高さx幅（strideとpaddingから計算される値）\n",
    "  - 1番目の畳み込み層\n",
    "    - 入力サイズ = 320 x 704\n",
    "    - 出力サイズ = 160 x 352\n",
    "      - (320-3+2*1)/2 + 1 = 160 \n",
    "      - (704-3+2*1)/2 + 1 = 352\n",
    "    \n",
    "  - 2.畳み込み層\n",
    "    - 入力サイズ = 160 x 352\n",
    "    - 出力サイズ = 80  x 176\n",
    "  \n",
    "  - 3.畳み込み層\n",
    "    - 入力サイズ = 80 x 176\n",
    "    - 出力サイズ = 40 x  88\n",
    "\n",
    "  - 4.畳み込み層\n",
    "    - 入力サイズ = 40 x 88\n",
    "    - 出力サイズ = 20 x 44\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 下図の Lidarブランチに関する補足\n",
    "### 入力チャンネル = 中身は前処理によって異なる = 3\n",
    "  - チャンネル1：距離情報（例：地面からの高さ）\n",
    "  - チャンネル2：強度情報（例：反射強度）\n",
    "  - チャンネル3：分類情報（例：地面、障害物、車両などのカテゴリ）\n",
    "  \n",
    "### 変換後のブロックサイズ＝64 x 64 x 72\n",
    "  - 64 = 変換後の特徴マップの高さ\n",
    "  - 64 = 変換後の特徴マップの幅\n",
    "  - 72 = 変換後の特徴チャンネル数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encodingの補足\n",
    "![](../Images/PositionalEncoding01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 32, 512])\n",
      "tensor([[0., 1., 0.,  ..., 1., 0., 1.],\n",
      "        [0., 1., 0.,  ..., 1., 0., 1.],\n",
      "        [0., 1., 0.,  ..., 1., 0., 1.],\n",
      "        ...,\n",
      "        [0., 1., 0.,  ..., 1., 0., 1.],\n",
      "        [0., 1., 0.,  ..., 1., 0., 1.],\n",
      "        [0., 1., 0.,  ..., 1., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Positional Encodingクラスの定義\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    PositionalEncodingクラスは、Transformerモデルにおいて位置情報をエンコードするために使用されます。\n",
    "    このクラスは、固定長のサイン波とコサイン波を使用して位置エンコーディングを計算します。\n",
    "\n",
    "    引数:\n",
    "    d_model (int): 埋め込み次元の数（データの各位置についての特徴ベクトルのサイズ）。\n",
    "    max_len (int): エンコードする最大シーケンス長。デフォルトは5000。\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        \"\"\"\n",
    "        PositionalEncodingクラスのコンストラクタ。\n",
    "\n",
    "        引数:\n",
    "        d_model (int): 埋め込み次元の数。\n",
    "        max_len (int): エンコードする最大シーケンス長。デフォルトは5000。\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # [max_len , d_model] のゼロテンソルを作成\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        \n",
    "        # 分子. 位置エンコーディングの計算に使用される位置インデックス. unsqueeze(1)で2D Matrixにする\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # 分母. エンコーディングの周波数スケールを決定するためのスケーリング係数を計算\n",
    "        div_term = torch.exp(torch.arange(0, d_model,2).float() * (torch.log(torch.tensor(10000.0)) / d_model  ))\n",
    "        \n",
    "        # 偶数インデックスには、Sin\n",
    "        self.encoding[:, 0::2] = torch.sin(position / div_term)\n",
    "        \n",
    "        # 奇数インデックスには、Cos\n",
    "        self.encoding[:, 1::2] = torch.cos(position / div_term)\n",
    "        \n",
    "        # バッチサイズ次元を追加\n",
    "        self.encoding = self.encoding.unsqueeze(1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        入力テンソルに位置エンコーディングを追加する。\n",
    "\n",
    "        引数:\n",
    "        x (Tensor): 入力テンソル。サイズは (sequence length, batch size, d_model) を想定。\n",
    "\n",
    "        戻り値:\n",
    "        Tensor: 位置エンコーディングが追加された入力テンソル。\n",
    "        \"\"\"\n",
    "        length = x.size(0)\n",
    "        return x + self.encoding[:length, :]\n",
    "    \n",
    "if (True):    \n",
    "    # 使用例\n",
    "    d_model = 512  # 埋め込み次元の数\n",
    "    max_len = 100  # エンコードする最大シーケンス長\n",
    "\n",
    "    # PositionalEncodingインスタンスを作成\n",
    "    pos_enc = PositionalEncoding(d_model, max_len)\n",
    "\n",
    "    # ダミー入力データ (sequence length, batch size, d_model)\n",
    "    dummy_input = torch.zeros(50, 32, d_model)\n",
    "\n",
    "    # 位置エンコーディングを追加\n",
    "    output = pos_enc(dummy_input)\n",
    "    print(output.size())  # 出力のサイズを表示\n",
    "    print(output[0])    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfuserクラスの定義\n",
    "class TransFuser(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransFuser, self).__init__()\n",
    "        \n",
    "        # RGB画像のブランチ: nn.Conv2d(入力チャネル、出力チャネル、...)\n",
    "        # 最初は、RGBの3チャンネルが入力となる\n",
    "        self.img_conv1 = nn.Conv2d(  3,   72, kernel_size=3, stride=2, padding=1)\n",
    "        self.img_conv2 = nn.Conv2d( 72,  216, kernel_size=3, stride=2, padding=1)\n",
    "        self.img_conv3 = nn.Conv2d(216,  576, kernel_size=3, stride=2, padding=1)\n",
    "        self.img_conv4 = nn.Conv2d(576, 1512, kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # LiDARブランチ\n",
    "        self.lidar_conv1 = nn.Conv2d(  3,   72, kernel_size=3, stride=2, padding=1)\n",
    "        self.lidar_conv2 = nn.Conv2d( 72,  216, kernel_size=3, stride=2, padding=1)\n",
    "        self.lidar_conv3 = nn.Conv2d(216,  576, kernel_size=3, stride=2, padding=1)\n",
    "        self.lidar_conv4 = nn.Conv2d(576, 1512, kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # Positional Encoding : 埋め込み次元(d_model) は チャンネル数に合わせる\n",
    "        # 埋め込み次元は、MLやNLPにおいて、高次元の入力データを低次元の連続空間に変換するためのベクトル表現の次元数\n",
    "        self.positional_encoding1 = PositionalEncoding(d_model =   72)\n",
    "        self.positional_encoding2 = PositionalEncoding(d_model =  216)\n",
    "        self.positional_encoding3 = PositionalEncoding(d_model =  576)\n",
    "        self.positional_encoding4 = PositionalEncoding(d_model = 1512)\n",
    "        \n",
    "        # TransformerEncoderLayer\n",
    "        # --> 自己注意メカニズム(self-attention)とフィードフォワードネットワークで構成されたTransformerの基本的な構成要素\n",
    "        #     マルチヘッドアテンション（Multi-Head Attention）のヘッド数(nhead)とは、\n",
    "        #     アテンションメカニズムにおいて同時に複数の異なる部分空間でアテンションを計算するためのヘッドの数を指します。\n",
    "        encoder_layers1 = TransformerEncoderLayer(d_model=  72, nhead=8)\n",
    "        encoder_layers2 = TransformerEncoderLayer(d_model= 216, nhead=8)\n",
    "        encoder_layers3 = TransformerEncoderLayer(d_model= 576, nhead=8)\n",
    "        encoder_layers4 = TransformerEncoderLayer(d_model=1512, nhead=8)\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        #   各解像度において、上記の self-attentionは、1層ずつモデル化している\n",
    "        #   全ての解像度を通過すると、4個の self-attention層を通過したことになる\n",
    "        self.transformer1 = TransformerEncoder(encoder_layers1, num_layers=1)\n",
    "        self.transformer2 = TransformerEncoder(encoder_layers2, num_layers=1)\n",
    "        self.transformer3 = TransformerEncoder(encoder_layers3, num_layers=1)\n",
    "        self.transformer4 = TransformerEncoder(encoder_layers4, num_layers=1)\n",
    "        \n",
    "        # 最終層\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3_11_UdemyLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
