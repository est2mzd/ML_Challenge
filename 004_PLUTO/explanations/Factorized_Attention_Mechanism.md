### 因子化された注意機構の目的
因子化された注意機構（Factorized Attention Mechanism）は、自己注意機構（Self-Attention）の計算効率を向上させるために採用される手法です。通常の自己注意機構では、入力の長さに対して計算量が二乗のオーダーで増加しますが、因子化することで計算量を削減し、処理を高速化します。

### 自己注意機構の基本
自己注意機構は、入力データの各要素が他の全ての要素に対して「注意を向ける」ことで、入力間の相関関係を学習します。このプロセスは通常、以下のように計算されます：

1. **クエリ（Query）**: 各要素が他の要素にどれだけ注意を払うべきかを決定するためのベクトル。
2. **キー（Key）**: 他の要素からクエリに関連する情報を取得するためのベクトル。
3. **バリュー（Value）**: クエリが注意を払うべき情報自体を表すベクトル。

通常、自己注意機構の計算量は入力の長さ $N$ に対して $O(N^2)$ となります。

### 因子化された注意機構の仕組み
因子化された注意機構は、この計算量を削減するために次のように処理を分割します：

1. **軸ごとに分割**:
   - クエリとキーの組み合わせを異なる軸（例えば、横方向と縦方向）に分割します。
   - 具体的には、横方向クエリ $Q_{\text{lat}}$ と縦方向クエリ $Q_{\text{lon}}$ を使って、それぞれの軸に対して個別に注意機構を計算します。

2. **計算量の削減**:
   - この因子化により、計算量が $O(N^2)$ から $O(N \times M)$ に削減されます。ここで、$N$ と $M$ はそれぞれ分割された軸に対応する長さです。

### なぜ因子化が重要なのか？
因子化された注意機構を使用することには以下の利点があります：

1. **計算効率の向上**:
   - 入力データが大規模な場合でも、因子化によって計算量が削減され、リアルタイム処理が可能になります。

2. **メモリ使用量の削減**:
   - 計算量の削減により、メモリ使用量も減少します。これにより、大規模データの処理が可能になります。

3. **適用可能なシナリオの拡大**:
   - 因子化された注意機構は、空間的なデータや高次元データの処理に特に効果的であり、自動運転や画像処理の分野で広く採用されています。

### まとめ
因子化された注意機構は、自己注意機構の計算量を効果的に削減しつつ、その機能を保持するための手法です。入力データを軸ごとに分割し、それぞれに対して注意を計算することで、効率的かつ高速な処理を実現します。これにより、複雑なデータを扱う際の計算負荷が大幅に軽減されます。

---
---
### 因子化とは

### 因子化のイメージ
因子化の概念は、通常の計算処理を以下のように分割して行うことに似ています：

- **通常のアプローチ**: 全ての要素を一度に処理する。
- **因子化されたアプローチ**: 要素を部分ごとに分割し、それぞれを個別に処理した後、結果を統合する。

例えば、文章の全単語に対して一度に注意を計算するのではなく、文章を文節ごとに分けてそれぞれの文節に対して注意を計算し、その結果を後で統合するイメージです。


因子化された注意機構では、以下のステップで計算が行われます：

1. **クエリとキーの分解**:
   - 通常の自己注意機構では、クエリ（Query）とキー（Key）を用いて全ての要素間で注意を計算しますが、因子化された注意機構ではこれを「軸ごと」に分割して計算します。
   - 例えば、横方向の情報（Lateral Information）と縦方向の情報（Longitudinal Information）を分けて、それぞれ独立に注意を計算します。

2. **横方向と縦方向の計算**:
   - 横方向クエリ $Q_{\text{lat}}$ は横方向のキーと、縦方向クエリ $Q_{\text{lon}}$ は縦方向のキーとそれぞれに対して注意を計算します。
   - この際、それぞれの計算は独立して行われるため、全体の計算量が大幅に削減されます。

3. **計算量の削減**:
   - 通常、全要素に対して自己注意を適用すると、計算量は $O(N^2)$ となりますが、因子化を行うと計算が分割され、計算量は $O(N \times M)$ となります。ここで、$N$ と $M$ はそれぞれ分割された軸に対応する長さです。
   - 例えば、横方向の軸には $N_R$、縦方向の軸には $N_L$ が対応し、計算量が $O(N_R^2 N_L)$ や $O(N_R N_L^2)$ に削減されます。

4. **最終的な統合**:
   - 各軸ごとに計算された注意を統合して、最終的な結果を得ます。この統合により、全体の情報が効果的にまとめられますが、計算量は因子化の効果で削減されたままとなります。



### まとめ
因子化された注意機構では、通常の自己注意機構の計算を軸ごとに分割し、それぞれに対して独立に注意を計算することで、全体の計算量を削減します。この手法により、大規模なデータや高次元のデータに対しても効率的に処理を行うことができるようになります。
