'''

以下は、Cross Attentionの例を具体的に示すために、先ほどのクエリ、キー、バリューの概念を使ったPyTorchのプログラムです。
Cross Attentionでは、クエリ、キー、バリューが異なる入力から派生します。
ここでは、デコーダのクエリがエンコーダの出力に注意を向ける例を示します。

### コードの説明

1. **エンコーダの出力 (Keys, Values)**:
   - 倉庫にある商品の特徴ベクトルをエンコーダの出力と見なします。キー (`keys`) は商品の特徴を表し、バリュー (`values`) は商品の価格を表します。

2. **デコーダのクエリ (Queries)**:
   - デコーダのクエリは顧客の要求する特徴を表しています。例えば、「赤い色で重さが0.5kg程度のもの」を探している場合、その特徴ベクトルがクエリとなります。

3. **クエリとキーの類似度を計算**:
   - クエリとエンコーダの出力(キー)の間で内積を取ることで、クエリが各キー(倉庫の商品)とどれだけ似ているかを計算します。この内積の結果として、各クエリがどのキーに最も関連しているかが示されます。

4. **スケーリング**:
   - 類似度スコアをキーの次元数の平方根で割ってスケーリングします。これにより、スコアが過度に大きくならないように調整されます。

5. **ソフトマックス関数で正規化**:
   - スケーリングされたスコアをソフトマックス関数で正規化し、注意の重みを計算します。これにより、どの商品の特徴がクエリに対して最も関連性が高いかが決まります。

6. **バリューに注意の重みを適用**:
   - 計算された注意の重みを使って、各商品の価格(バリュー)を重み付けします。これにより、クエリに最も関連性の高い商品の価格が強調され、最終的な出力が計算されます。

### 実行結果の解釈

- **Similarity Scores**:
  - クエリ(顧客の要求する特徴)とキー(倉庫にある商品の特徴)との類似度を示します。例えば、赤い色で重さが似ている商品は高いスコアを持ちます。

- **Scaled Scores**:
  - 類似度スコアをスケーリングしたものです。

- **Attention Weights**:
  - 各商品の特徴に対する注意の重みです。特定のキーがクエリに対してどれだけ関連性があるかを示しています。

- **Weighted Value (Output)**:
  - 最終的な出力です。クエリに最も関連する商品の価格情報が反映されます。これがデコーダの出力として使用されます。

### まとめ
このCross Attentionの例では、クエリ(顧客の要求)がエンコーダの出力(倉庫にある商品情報)に対してどれだけ関連しているかを計算し、
その関連性に基づいてバリュー(商品の価格)を重み付けして出力を生成しています。これにより、顧客の要求に最も関連する商品の価格が選択される仕組みを理解することができます。
'''

import torch
import torch.nn.functional as F

# エンコーダの出力として、倉庫にある商品の特徴ベクトル (Keys, Values) を考えます。
# キーとバリューはエンコーダの出力に対応し、クエリはデコーダの入力に対応します。

# 商品の特徴量: 色 (Red, Green, Blue) と重さ (Weight)
# エンコーダの出力 (倉庫にある商品情報)
keys = torch.tensor([
    [1.0, 0.0, 0.0, 0.4],  # 商品1: 赤い色、少し軽い (Red=1, Green=0, Blue=0, Weight=0.4)
    [0.0, 1.0, 0.0, 0.6],  # 商品2: 緑色、少し重い (Red=0, Green=1, Blue=0, Weight=0.6)
    [1.0, 0.0, 0.0, 0.7]   # 商品3: 赤い色、少し重い (Red=1, Green=0, Blue=0, Weight=0.7)
])  # サイズ: (3, 4)

# バリュー: キーに対応する商品の価格（例えば、商品の値段を意味する）
values = torch.tensor([
    [100.0],  # 商品1の価格
    [200.0],  # 商品2の価格
    [150.0]   # 商品3の価格
])  # サイズ: (3, 1)

# デコーダのクエリ (顧客の要求する特徴)
# 例えば、顧客が「赤い商品で重さが0.5kg程度のもの」を探している場合
queries = torch.tensor([
    [1.0, 0.0, 0.0, 0.5],  # クエリ: 赤い色、重さ0.5kg (Red=1, Green=0, Blue=0, Weight=0.5)
])  # サイズ: (1, 4)

# クエリとキーの類似度を計算 (内積を取る)
similarity_scores = torch.matmul(queries, keys.T)  # サイズ: (1, 3)
print(f"Similarity Scores:\n{similarity_scores}")

# スケーリング (キーの次元数の平方根で割る)
d_k = keys.size(1)  # キーの次元数 (この場合は4)
scaled_scores = similarity_scores / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))
print(f"Scaled Scores:\n{scaled_scores}")

# ソフトマックス関数で正規化して注意の重みを得る
attention_weights = F.softmax(scaled_scores, dim=-1)  # サイズ: (1, 3)
print(f"Attention Weights:\n{attention_weights}")

# 注意の重みをバリューに適用する
weighted_values = torch.matmul(attention_weights, values)  # サイズ: (1, 1)
print(f"Weighted Value (Output):\n{weighted_values}")
