### Self Attention と Cross Attention の違い

**Self Attention** と **Cross Attention** は、Transformerモデルにおける重要なメカニズムであり、それぞれ異なる役割を持っています。これらの違いについて、先ほどの式を使って丁寧に説明します。

### 1. Self Attention (自己注意)

**Self Attention** は、入力シーケンス内の各トークンが他のすべてのトークンに対して「注意」を向けるメカニズムです。これにより、各トークンがシーケンス全体の情報を考慮しながら、そのトークンに対する出力を生成します。

#### Self Attention の計算式

$$
\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
$$

この式において、$Q$（クエリ）、$K$（キー）、$V$（バリュー）はすべて**同じシーケンス**から生成されます。具体的には、Self Attention では次のようになります：

- **Q, K, V** は同じ入力シーケンス（例えば、エンコーダの入力またはデコーダの入力）から派生しています。
- 各トークンが他のすべてのトークンに対してどう注意を向けるべきかを決定し、その注意の重みに基づいて自分自身の情報を再構築します。

#### Self Attention の目的

Self Attention は、入力シーケンス内のトークン間の依存関係を捉えるために使用されます。例えば、文中の単語が他の単語とどのように関係しているかを学習し、文脈に基づいて各単語の特徴を更新します。

### 2. Cross Attention (クロスアテンション)

**Cross Attention** は、**異なるシーケンス間**で「注意」を向けるメカニズムです。具体的には、通常デコーダで使用され、デコーダのトークンがエンコーダの出力シーケンスに注意を向ける際に使用されます。

#### Cross Attention の計算式

$$
\text{head}_i = \text{Attention}(QW_i^Q, K_{\text{enc}}W_i^K, V_{\text{enc}}W_i^V)
$$

この式において、**クエリ** $Q$ はデコーダの入力シーケンスから生成され、**キー** $K_{\text{enc}}$ と **バリュー** $V_{\text{enc}}$ はエンコーダの出力シーケンスから生成されます。

- **Q** はデコーダの入力シーケンス（例えば、翻訳の出力シーケンスの一部）です。

- **K_{\text{enc}}** と **V_{\text{enc}}** はエンコーダの出力（エンコーダが入力シーケンスから生成した特徴ベクトル）です。

#### Cross Attention の目的

Cross Attention は、デコーダが生成しようとしているトークン（例えば、翻訳文の単語）が
