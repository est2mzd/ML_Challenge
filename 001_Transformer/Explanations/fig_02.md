# 図2の左側（Scaled Dot-Product Attention）についての詳細説明

![fig1](../Images/fig_02.png)

## Scaled Dot-Product Attentionの概要
**Scaled Dot-Product Attention**は、クエリ (Q)、キー (K)、バリュー (V) という3つの入力テンソルを用いて、入力シーケンスの異なる部分間の関係性を捉えるための基本的な注意メカニズムです。この注意メカニズムは、クエリとキーの間の内積を計算し、キーの次元数でスケーリングした後にソフトマックス関数を適用することで、バリューに対する重みを求めます。

## 数式の説明
Scaled Dot-Product Attentionの計算は以下の式で表されます：

```math
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V
```

ここで、各項目は以下の意味を持ちます：

- **Q (クエリ)**: 注意を向ける対象を示すベクトル。通常、入力シーケンスの一部または特徴ベクトルを意味します。
- **K (キー)**: 各クエリと照合されるベクトル。通常、入力シーケンスの各要素に対応します。
- **V (バリュー)**: キーに対応する出力ベクトル。最終的に加重和を取られることでAttentionの結果として出力されます。
- **$ d_k$**: キーの次元数。この次元数でスケーリングを行うことで、内積の値を適切な範囲に調整し、勾配消失や勾配爆発を防ぎます。

## 図中の各ブロックの説明

1. **MatMul (行列積)**
   - **説明**: 最初に、クエリ (Q) とキー (K) の転置行列との行列積を計算します。これにより、クエリとキーの間の類似度を示すスコア行列が得られます。
   - **数式**: $QK^\top$

2. **Scale (スケーリング)**
   - **説明**: 次に、このスコア行列をキーの次元数 ($ d_k$) の平方根で割ります。これは、キーの次元数が大きくなるにつれてスコアの値が極端に大きくなるのを防ぐためです。
   - **数式**: $\frac{QK^\top}{\sqrt{d_k}}$

3. **Mask (マスキング、オプション)**
   - **説明**: このステップはオプションで、特にデコーダで使用されます。ここでは、将来の情報に基づく注意を避けるために、未来の位置に対応するスコアにマスク（極小値）を適用します。
   - **数式**: このステップは実際の計算式には影響を与えないが、特定の位置に対するスコアを無視するために使用されます。

4. **SoftMax**
   - **説明**: スケーリングされたスコア行列にソフトマックス関数を適用して、各キーに対する注意の重みを確率分布として計算します。この重みが最終的にバリューに対して適用されます。
   - **数式**: $\text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)$

5. **MatMul (行列積)**
   - **説明**: 最後に、計算された注意の重みをバリュー (V) に掛け算することで、最終的なAttentionの出力を得ます。この出力が次の層に渡されます。
   - **数式**: $\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V$

## 結論
Scaled Dot-Product Attentionは、クエリとキーの間の類似度を計算し、その結果に基づいてバリューを加重平均することで、入力シーケンス内の重要な部分に動的に注意を向ける仕組みです。このプロセスにより、モデルはシーケンス内のどの部分に焦点を当てるべきかを学習し、特定のタスク（例：翻訳や質問応答）に最適化された出力を生成します。

---

# 図2の右側（Multi-Head Attention）についての詳細説明

![fig1](../Images/fig_02.png)

## Multi-Head Attentionの概要
**Multi-Head Attention**は、複数の注意機構（Attention Heads）を並列に実行することで、異なる位置にあるトークン間の依存関係を複数の異なる表現空間で同時に捉えるための手法です。各ヘッドは独自のSelf-Attention Mechanismを実行し、それぞれの出力を結合することで、モデルが異なる観点から情報を集約できるようにします。

## 数式の説明
Multi-Head Attentionの計算は以下の式で表されます：

```math
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \dots, \text{head}_h)W^O
```

ここで、各ヘッド $\text{head}_i$ は次のように計算されます：

```math
\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
```

各項目の意味は以下の通りです：

- **Q (クエリ)**, **K (キー)**, **V (バリュー)**: これらは入力に基づいて計算されるテンソルで、それぞれ異なる重み行列 ($W_i^Q$, $W_i^K$, $W_i^V$) を掛けることで変換されます。
- **$W_i^Q$, $W_i^K$, $W_i^V$**: 各注意ヘッドに対応するクエリ、キー、バリューの重み行列です。これらは学習可能なパラメータです。
- **$W^O$**: 各ヘッドの出力を結合した後に適用される線形変換の重み行列です。

## 図中の各ブロックの説明

1. **Linear (線形変換)**
   - **説明**: 入力のクエリ (Q)、キー (K)、バリュー (V) は、それぞれ異なる重み行列 ($W_i^Q$, $W_i^K$, $W_i^V$) を使用して、複数のAttention Headsに対応するサブスペースに投影されます。これにより、各ヘッドが異なる特徴空間でのAttentionを学習できるようになります。
   - **数式**: $ QW_i^Q$, $ KW_i^K$, $ VW_i^V$

2. **Scaled Dot-Product Attention**
   - **説明**: 各ヘッドは、独自のSelf-Attention Mechanismを実行します。これは前述のScaled Dot-Product Attentionの計算に基づいています。各ヘッドがクエリ、キー、バリューを用いて注意スコアを計算し、対応するバリューを重み付けします。
   - **数式**: $ \text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$

3. **Concat (結合)**
   - **説明**: 各ヘッドの出力を結合します。これにより、複数の異なる視点から得られた注意の結果が1つのテンソルに統合されます。
   - **数式**: $ \text{Concat}(\text{head}_1, \dots, \text{head}_h)$

4. **Linear (線形変換)**
   - **説明**: 結合された出力に対して、最終的な線形変換が適用されます。この変換は、元の入力と同じ次元数に戻すために行われます。
   - **数式**: $ \text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \dots, \text{head}_h)W^O$

## 結論
Multi-Head Attentionは、複数のSelf-Attention Mechanismを並列に実行し、その結果を統合することで、より豊かな表現力を持つ注意機構を提供します。各ヘッドは異なるサブスペースでAttentionを実行するため、モデルは異なる特徴を捉えつつ、シーケンス内の複雑な依存関係を同時に学習することが可能です。これにより、Transformerは高いパフォーマンスを発揮できるようになっています。
