### 残差接続の基本的な仕組み

残差接続とは、ある層の出力に対して、その層の入力を直接加算する接続のことを指します。具体的には、ある層の入力を \( x \) とし、その層で得られた出力を \( F(x) \) とすると、残差接続を持つ層の出力は次のように計算されます。

\[
\text{出力} = F(x) + x
\]

このシンプルな加算により、ネットワークは残差を学習することが可能となり、直接的な情報の伝達が可能になります。

### 残差接続の効果

#### a. 勾配消失・勾配爆発の防止

深層ネットワークにおける大きな問題の一つは、層が深くなるにつれて勾配が消失したり、逆に勾配が爆発することです。これにより、ネットワークのトレーニングが困難になることがあります。残差接続を導入することで、勾配が直接的にバックプロパゲーションされる経路が確保されるため、勾配消失や勾配爆発が軽減されます。

#### b. 学習の安定化と高速化

残差接続により、ネットワークの層が「恒等写像」（何もしない層）として振る舞うことが容易になります。これは、ネットワークが層の学習を避けることができるため、トレーニング初期においても有効です。その結果、学習が安定化し、ネットワークの収束速度が向上します。

#### c. 情報のスムーズな伝達

残差接続は、層を飛び越えて情報をスムーズに伝達する手段を提供します。これにより、深い層の情報が劣化することなく、入力情報が出力に反映されやすくなります。特に、重要な特徴が深い層で消失することなく、出力まで伝わるようになります。

#### d. 深層モデルの有効性の向上

残差接続を用いることで、非常に深いモデルをトレーニングすることが可能になります。これにより、より複雑なパターンを学習する能力が向上し、モデルの性能が向上します。

### Transformer における残差接続

Transformerにおいては、各サブレイヤー（例えば、マルチヘッド・アテンションやフィードフォワード・ネットワーク）に対して残差接続が導入されています。これにより、次のような効果が得られます：

- **マルチヘッド・アテンション**: 自己注意メカニズムで得られた情報に対して、元の入力を加算することで、層を通じての情報の流れがスムーズになります。
- **フィードフォワード・ネットワーク**: 非線形変換後の出力に元の入力を加算することで、情報の一貫性が保たれます。

### まとめ

残差接続は、ディープラーニングモデルの学習を容易にし、トレーニングの安定性と速度を向上させるための強力なメカニズムです。特に深層ネットワークやTransformerのような複雑なモデルにおいて、残差接続は、勾配消失の問題を軽減し、情報の劣化を防ぐ重要な役割を果たしています。その結果、モデルの性能向上に寄与します。
